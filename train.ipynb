{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (hashtable as _hashtable,\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos, lib\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import hashing, tslib\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (lib, index as libindex, tslib as libts,\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.tslibs.offsets as liboffsets\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/core/ops.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos as libalgos, ops as libops\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/core/indexes/interval.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs.interval import (\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/core/internals.py:14: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import internals as libinternals\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/core/sparse/array.py:33: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.sparse as splib\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/core/window.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.window as _window\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/core/groupby/groupby.py:68: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (lib, reduction,\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/core/reshape/reshape.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos as _algos, reshape as _reshape\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/io/parsers.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.parsers as parsers\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/pandas/io/pytables.py:50: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos, lib, writers as libwriters\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n",
      "Using TensorFlow backend.\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._solve_toeplitz import levinson\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/linalg/__init__.py:207: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._decomp_update import *\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ufuncs import *\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/interpolate/_bsplines.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _bspl\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/sparse/lil.py:19: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _csparsetools\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:165: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._shortest_path import shortest_path, floyd_warshall, dijkstra,\\\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/sparse/csgraph/_validation.py:5: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._tools import csgraph_to_dense, csgraph_from_dense,\\\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:167: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._traversal import breadth_first_order, depth_first_order, \\\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:169: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._min_spanning_tree import minimum_spanning_tree\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/sparse/csgraph/__init__.py:170: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._reordering import reverse_cuthill_mckee, maximum_bipartite_matching, \\\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .ckdtree import *\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/spatial/__init__.py:96: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .qhull import *\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _voronoi\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/spatial/distance.py:122: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _hausdorff\n",
      "/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/scipy/ndimage/measurements.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _ni_label\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, GRU, TimeDistributed, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "df = pd.read_csv(\"yelp_2013.csv\")\n",
    "#df = df.sample(5000)\n",
    "\n",
    "Y = df.stars.values-1\n",
    "Y = to_categorical(Y,num_classes=5)\n",
    "X = df.text.values\n",
    "\n",
    "#set hyper parameters\n",
    "MAX_NUM_WORDS = 30000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT=0.1\n",
    "NUM_FILTERS = 50\n",
    "MAX_LEN = 512\n",
    "Batch_size = 100\n",
    "EPOCHS = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data\n",
    "indices = np.arange(X.shape[0])\n",
    "np.random.seed(2018)\n",
    "np.random.shuffle(indices)\n",
    "X=X[indices]\n",
    "Y=Y[indices]\n",
    "\n",
    "#training set, validation set and testing set\n",
    "nb_validation_samples_val = int((VALIDATION_SPLIT + TEST_SPLIT) * X.shape[0])\n",
    "nb_validation_samples_test = int(TEST_SPLIT * X.shape[0])\n",
    "\n",
    "x_train = X[:-nb_validation_samples_val]\n",
    "y_train = Y[:-nb_validation_samples_val]\n",
    "x_val =  X[-nb_validation_samples_val:-nb_validation_samples_test]\n",
    "y_val =  Y[-nb_validation_samples_val:-nb_validation_samples_test]\n",
    "x_test = X[-nb_validation_samples_test:]\n",
    "y_test = Y[-nb_validation_samples_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tokenizer to build vocab\n",
    "tokenizer1 = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer1.fit_on_texts(df.text)\n",
    "vocab = tokenizer1.word_index\n",
    "\n",
    "x_train_word_ids = tokenizer1.texts_to_sequences(x_train)\n",
    "x_test_word_ids = tokenizer1.texts_to_sequences(x_test)\n",
    "x_val_word_ids = tokenizer1.texts_to_sequences(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"we've bought two vehicles from saul aguirre and can't recommend him enough!  had some service done recently and service adviser mike d. was very professional and attentive, and kept in excellent contact. service manager also was quick to address my initial concerns when i arrived.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[890,\n",
       " 690,\n",
       " 129,\n",
       " 4226,\n",
       " 48,\n",
       " 2,\n",
       " 202,\n",
       " 190,\n",
       " 250,\n",
       " 226,\n",
       " 23,\n",
       " 62,\n",
       " 47,\n",
       " 251,\n",
       " 681,\n",
       " 2,\n",
       " 47,\n",
       " 18717,\n",
       " 2443,\n",
       " 787,\n",
       " 6,\n",
       " 35,\n",
       " 610,\n",
       " 2,\n",
       " 543,\n",
       " 2,\n",
       " 623,\n",
       " 11,\n",
       " 241,\n",
       " 1746,\n",
       " 47,\n",
       " 390,\n",
       " 70,\n",
       " 6,\n",
       " 382,\n",
       " 5,\n",
       " 2466,\n",
       " 12,\n",
       " 2496,\n",
       " 2984,\n",
       " 50,\n",
       " 3,\n",
       " 478]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_word_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad sequences into the same length\n",
    "x_train_padded_seqs = pad_sequences(x_train_word_ids, maxlen=MAX_LEN)\n",
    "x_test_padded_seqs = pad_sequences(x_test_word_ids, maxlen=MAX_LEN)\n",
    "x_val_padded_seqs = pad_sequences(x_val_word_ids, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0   890   690   129  4226    48     2   202   190   250   226    23\n",
      "    62    47   251   681     2    47 18717  2443   787     6    35   610\n",
      "     2   543     2   623    11   241  1746    47   390    70     6   382\n",
      "     5  2466    12  2496  2984    50     3   478]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_padded_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice sequences into many subsequences\n",
    "x_test_padded_seqs_split=[]\n",
    "for i in range(x_test_padded_seqs.shape[0]):\n",
    "    split1=np.split(x_test_padded_seqs[i],8)\n",
    "    a=[]\n",
    "    for j in range(8):\n",
    "        s=np.split(split1[j],8)\n",
    "        a.append(s)\n",
    "    x_test_padded_seqs_split.append(a)\n",
    "\n",
    "x_val_padded_seqs_split=[]\n",
    "for i in range(x_val_padded_seqs.shape[0]):\n",
    "    split1=np.split(x_val_padded_seqs[i],8)\n",
    "    a=[]\n",
    "    for j in range(8):\n",
    "        s=np.split(split1[j],8)\n",
    "        a.append(s)\n",
    "    x_val_padded_seqs_split.append(a)\n",
    "   \n",
    "x_train_padded_seqs_split=[]\n",
    "for i in range(x_train_padded_seqs.shape[0]):\n",
    "    split1=np.split(x_train_padded_seqs[i],8)\n",
    "    a=[]\n",
    "    for j in range(8):\n",
    "        s=np.split(split1[j],8)\n",
    "        a.append(s)\n",
    "    x_train_padded_seqs_split.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "  array([  0,   0,   0,   0,   0, 890, 690, 129], dtype=int32),\n",
       "  array([4226,   48,    2,  202,  190,  250,  226,   23], dtype=int32),\n",
       "  array([   62,    47,   251,   681,     2,    47, 18717,  2443],\n",
       "        dtype=int32),\n",
       "  array([787,   6,  35, 610,   2, 543,   2, 623], dtype=int32),\n",
       "  array([  11,  241, 1746,   47,  390,   70,    6,  382], dtype=int32),\n",
       "  array([   5, 2466,   12, 2496, 2984,   50,    3,  478], dtype=int32)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_padded_seqs_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GloVe embeddings\n",
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#load pre-trained GloVe word embeddings\n",
    "print \"Using GloVe embeddings\"\n",
    "glove_path = 'glove.6B.200d.txt'\n",
    "embeddings_index = {}\n",
    "f = open(glove_path)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pre-trained GloVe word embeddings to initialize the embedding layer\n",
    "embedding_matrix = np.random.random((MAX_NUM_WORDS + 1, EMBEDDING_DIM))\n",
    "for word, i in vocab.items():\n",
    "    if i<MAX_NUM_WORDS:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "        # words not found in embedding index will be random initialized.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "embedding_layer = Embedding(MAX_NUM_WORDS + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_LEN/64,\n",
    "                            trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Model\n"
     ]
    }
   ],
   "source": [
    "#build model\n",
    "print \"Build Model\"\n",
    "input1 = Input(shape=(MAX_LEN/64,), dtype='int32')\n",
    "embed = embedding_layer(input1)\n",
    "gru1 = GRU(NUM_FILTERS,recurrent_activation='sigmoid',activation=None,return_sequences=False)(embed)\n",
    "Encoder1 = Model(input1, gru1)\n",
    "\n",
    "input2 = Input(shape=(8,MAX_LEN/64,), dtype='int32')\n",
    "embed2 = TimeDistributed(Encoder1)(input2)\n",
    "gru2 = GRU(NUM_FILTERS,recurrent_activation='sigmoid',activation=None,return_sequences=False)(embed2)\n",
    "Encoder2 = Model(input2,gru2)\n",
    "\n",
    "input3 = Input(shape=(8,8,MAX_LEN/64), dtype='int32')\n",
    "embed3 = TimeDistributed(Encoder2)(input3)\n",
    "gru3 = GRU(NUM_FILTERS,recurrent_activation='sigmoid',activation=None,return_sequences=False)(embed3)\n",
    "preds = Dense(5, activation='softmax')(gru3)\n",
    "model = Model(input3, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 8, 200)            6000200   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 50)                37650     \n",
      "=================================================================\n",
      "Total params: 6,037,850\n",
      "Trainable params: 6,037,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 8, 50)             6037850   \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 50)                15150     \n",
      "=================================================================\n",
      "Total params: 6,053,000\n",
      "Trainable params: 6,053,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 8, 50)             6053000   \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 50)                15150     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 6,068,405\n",
      "Trainable params: 6,068,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print Encoder1.summary()\n",
    "print Encoder2.summary()\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use adam optimizer\n",
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the best model on validation set\n",
    "from keras.callbacks import ModelCheckpoint             \n",
    "savebestmodel = 'save_model/SRNN(8,2)_yelp2013.h5'\n",
    "checkpoint = ModelCheckpoint(savebestmodel, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks=[checkpoint] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh/entailment/srnn/srnn/srnn/lib/python2.7/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 374887 samples, validate on 46861 samples\n",
      "Epoch 1/10\n",
      " 21500/374887 [>.............................] - ETA: 4:55 - loss: 1.2235 - acc: 0.4639"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bfd2510ece7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           verbose = 1)\n\u001b[0m",
      "\u001b[0;32m/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/entailment/srnn/srnn/srnn/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(np.array(x_train_padded_seqs_split), y_train, \n",
    "          validation_data = (np.array(x_val_padded_seqs_split), y_val),\n",
    "          nb_epoch = EPOCHS, \n",
    "          batch_size = Batch_size,\n",
    "          callbacks = callbacks,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the best model to evaluate on test set\n",
    "from keras.models import load_model\n",
    "best_model= load_model(savebestmodel)          \n",
    "print best_model.evaluate(np.array(x_test_padded_seqs_split),y_test,batch_size=Batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srnn",
   "language": "python",
   "name": "srnn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
